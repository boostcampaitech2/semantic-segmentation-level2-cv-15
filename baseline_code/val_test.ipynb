{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(target):\n",
    "    # target 에 존재하는 train.json 파일을 엽니다.\n",
    "    with open(target, 'r') as f:\n",
    "        json_datas = json.load(f) # python dict 처럼 접근하게끔 변환\n",
    "    \t#dict_keys(['info', 'licenses', 'images', 'categories', 'annotations'])\n",
    "\n",
    "    category = {}\n",
    "    file_info = {}\n",
    "    make_frame = defaultdict(list)\n",
    "\n",
    "    # 이미지 정보 중 파일 경로와 아이디만 추출해서 file_info 에 저장\n",
    "    for item in json_datas['images']:\n",
    "        file_info[item['id']] = {'id' : item['id'], 'file_name' : item['file_name']}\n",
    "    # 카테고리 정보를 category 에 저장\n",
    "    for item in json_datas['categories']:\n",
    "        category[item['id']] = item['name']\n",
    "    # annotations 에 속하는 아이템들을 images 에 속하는 아이템의 정보와 합치기 위함\n",
    "    for annotation in json_datas['annotations']:\n",
    "        save_dict = file_info[annotation['image_id']]\n",
    "        # 각 이미지에 해당하는 bounding box 정보와 class 정보 area(넓이) 정보를 추가\n",
    "        bbox = np.array(annotation['bbox'])\n",
    "        bbox[2] = bbox[2] + bbox[0]\n",
    "        bbox[3] = bbox[3] + bbox[1]\n",
    "        save_dict.update({\n",
    "            'class': annotation['category_id'], # 배경은 0, 나머지 +1 in faster_rcnn\n",
    "            'x_min': bbox[0],\n",
    "            'y_min': bbox[1],\n",
    "            'x_max': bbox[2],\n",
    "            'y_max': bbox[3],\n",
    "            'area':annotation['area']\n",
    "            })\n",
    "\n",
    "        for k,v in save_dict.items():\n",
    "            # dataframe 으로 만들기 위해서 'key' : [item1,item2...] 형태로 저장\n",
    "            make_frame[k].append(v)\n",
    "\n",
    "    # dictionary 가 잘 만들어 졌는지 길이를 측정해서 확인해보세요!\n",
    "    print(len(json_datas['annotations']))\n",
    "    # dictionary to DataFrame\n",
    "    df = pd.DataFrame.from_dict(make_frame)\n",
    "    df.to_csv('./detection_info.csv',index=False)\n",
    "    print(df.head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26240\n",
      "   id             file_name  class  x_min  y_min  x_max  y_max     area\n",
      "0   0  batch_01_vt/0002.jpg      8  109.0  150.7  270.9  310.5   6307.5\n",
      "1   0  batch_01_vt/0002.jpg      8  413.2  196.1  485.8  248.8   3313.5\n",
      "2   0  batch_01_vt/0002.jpg      6   42.5  196.7  110.8  283.2   4741.0\n",
      "3   0  batch_01_vt/0002.jpg      5    0.1  279.7  117.4  462.7  18560.5\n",
      "4   0  batch_01_vt/0002.jpg      5  110.3   77.5  190.3  370.9   9759.0\n"
     ]
    }
   ],
   "source": [
    "annotation = '../input/data/train_all.json' # annotation 경로\n",
    "train_df = make_dataframe(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_group_k_fold\n",
    "\n",
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "\n",
    "    #stratified_group_k_fold(train_x, train_y, groups, k=5)\n",
    "\n",
    "    labels_num = np.max(y) + 1 # class num ()\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num)) # 그룹마다 클래스의 수 분포를 파악\n",
    "    y_distr = Counter() # 모든 라벨의 개수를 세어서 dict 형태로 반환 \n",
    "\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1 # 그룹마다 라벨의 위치에 클래스 개수를 늘려준다.\n",
    "        y_distr[label] += 1 # 총 라벨의 개수 증가\n",
    "    #print(y_distr)\n",
    "\n",
    "    #print(y_counts_per_group[\"train/4882.jpg\"])\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num)) # fold마다 클래스의 수 분포를 파악\n",
    "    groups_per_fold = defaultdict(set) # fold별 group 만들기\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts # fold 하나에 Image label이 더해집니다. (계산하기 위한 용도?, Numpy는 list끼리 더하기가 가능하다.\n",
    "        std_per_label = []\n",
    "        #print(y_counts_per_fold[fold])\n",
    "        for label in range(labels_num): # 10개\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]) # 모든 fold에 있는 라벨들에 대해 비율을 구하고 그 표준편차를 구한다.\n",
    "            std_per_label.append(label_std) # 라벨 당 표준편차들을 구합니다.\n",
    "            \n",
    "        y_counts_per_fold[fold] -= y_counts # fold당 모든 이미지의 라벨을 다시 뺴줍니다.\n",
    "        return np.mean(std_per_label)\n",
    "    \n",
    "    groups_and_y_counts = list(y_counts_per_group.items()) # list화  [file_name, label_list]\n",
    "    random.Random(seed).shuffle(groups_and_y_counts) # list random shuffle 섞을 필요가 있나? 밑에서 sort를 하는데..?\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])): # image당 라벨 개수들의 분포에 -표준편차를 기준으로 sort\n",
    "    #for g, y_counts in groups_and_y_counts:\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k): # k: fold 개수\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i) # group이 가지고 있는 라벨에서 / fold 별로 각 label의 표준 편차를 구하고 / 그룹에 있는 라벨들의 표준 편차의 평균을 구한다.\n",
    "            if min_eval is None or fold_eval < min_eval: # 라벨들의 표준편차 그리고 그에 평균 값이 가장 적은 곳에 best_fold를 해준다.\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "\n",
    "        # 결국 라벨들의 분포를 골고루 넣어주기 위한 과정\n",
    "\n",
    "        y_counts_per_fold[best_fold] += y_counts  # 폴드 당 라벨들의 합이 들어있따.\n",
    "        groups_per_fold[best_fold].add(g) # 폴드당 이미지들을 다 넣어줌\n",
    "\n",
    "    all_groups = set(groups) # 중복 그룹 제거\n",
    "    for i in range(k): # fold = 0, 1, 2, 3, 4 일 때,train/test group 만들기\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 에 존재하는 train.json 파일을 엽니다.\n",
    "\n",
    "def make_json(fold_ind, dev_id_json, val_id_json):\n",
    "\n",
    "    train_path = \"../input/data/train_all.json\"\n",
    "    train_file_name = f\"../dataset/fold_test/fold{fold_ind}_train.json\"\n",
    "    valid_file_name = f\"../dataset/fold_test/fold{fold_ind}_valid.json\"\n",
    "    \n",
    "    # print(fold_ind)\n",
    "    # print(dev_id_json[0])\n",
    "    # print(val_id_json[0])\n",
    "    with open(train_path, 'r') as f:\n",
    "        json_datas = json.load(f) # python dict 처럼 접근하게끔 변환\n",
    "    \t#dict_keys(['info', 'licenses', 'images', 'categories', 'annotations'])\n",
    "    # category = {}\n",
    "    # file_info = {}\n",
    "\n",
    "    info = json_datas[\"info\"]\n",
    "    licenses = json_datas[\"licenses\"]\n",
    "    categories = json_datas[\"categories\"]\n",
    "\n",
    "    images_train = []\n",
    "    images_valid = []\n",
    "\n",
    "    annotations_train = []\n",
    "    annotations_valid = []\n",
    "\n",
    "    for item in json_datas[\"images\"]:\n",
    "        if item[\"id\"] in dev_id_json:\n",
    "            images_train.append(item)\n",
    "        elif item[\"id\"] in val_id_json:\n",
    "            images_valid.append(item)\n",
    "        else:\n",
    "            print(\"no id in images\")\n",
    "\n",
    "\n",
    "    for item in json_datas[\"annotations\"]:\n",
    "        if item[\"image_id\"] in dev_id_json:\n",
    "            annotations_train.append(item)\n",
    "        elif item[\"image_id\"] in val_id_json:\n",
    "            annotations_valid.append(item)\n",
    "        else:\n",
    "            print(\"no id in annotations\")\n",
    "\n",
    "    make_json_train = defaultdict(list)\n",
    "    make_json_valid = defaultdict(list)\n",
    "    make_json_train = {\"info\":info, \"licenses\" : licenses, \"images\": images_train, \"categories\":categories, \"annotations\":annotations_train}\n",
    "    make_json_valid = {\"info\":info, \"licenses\" : licenses, \"images\": images_valid, \"categories\":categories, \"annotations\":annotations_valid}\n",
    "\n",
    "    # 개수 파악\n",
    "    if len(json_datas[\"images\"]) != len(make_json_train[\"images\"]) + len(make_json_valid[\"images\"]):\n",
    "        print(\"images length, diff\")\n",
    "\n",
    "    if len(json_datas[\"annotations\"]) != len(make_json_train[\"annotations\"]) + len(make_json_valid[\"annotations\"]):\n",
    "        print(\"annotations length, diff\")\n",
    "\n",
    "    # 이름 중복 파악 - > 모든 성분 파악\n",
    "    # for item in make_json_train[\"images\"] (dict):\n",
    "    #     if item[\"file_name\"] in make_json_valid[\"images\"]:\n",
    "    #         print(\"same file name exist\")\n",
    "    \n",
    "    # for item in make_json_train[\"annotations\"]:\n",
    "    #     if item[\"file_name\"] in make_json_valid[\"annotations\"]:\n",
    "    #         print(\"same file name exist\")\n",
    "\n",
    "    #assert len(set(make_json_train[\"annotations\"]) & set(make_json_valid[\"annotations\"])) == 0\n",
    "\n",
    "    #print(make_json)\n",
    "    \n",
    "    \n",
    "    with open(train_file_name, 'w') as output:\n",
    "        json.dump(make_json_train, output, indent=2)\n",
    "\n",
    "    with open(valid_file_name, 'w') as output:\n",
    "        json.dump(make_json_valid, output, indent=2)\n",
    "\n",
    "    # train, valid 개수랑 겹치는 부분\n",
    "\n",
    "    \n",
    "\n",
    "# fold0_train_path = \"/opt/ml/detection/dataset/fold_test/fold0_train.json\"\n",
    "# fold0_val_path = \"/opt/ml/detection/dataset/fold_test/fold0_val.json\"\n",
    "\n",
    "# with open(fold0_train_path, 'w') as f:\n",
    "#     json.dump(dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[\"id\"]\n",
    "train_y = train_df[\"class\"]\n",
    "groups = train_df[\"file_name\"]\n",
    "\n",
    "def get_distribution(y_vals):\n",
    "        y_distr = Counter(y_vals)\n",
    "        y_vals_sum = sum(y_distr.values())\n",
    "        return [f'{y_distr[i] / y_vals_sum:.2%}' for i in range(np.max(y_vals) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/segmentation/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    8\n",
      "1    8\n",
      "2    6\n",
      "3    5\n",
      "4    5\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9285/2470478362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdistrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'development set - fold {fold_ind}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdistrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9285/4283603017.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(y_vals)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my_distr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_vals_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'{y_distr[i] / y_vals_sum:.2%}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "distrs = [get_distribution(train_y)]\n",
    "index = ['training set']\n",
    "\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(stratified_group_k_fold(train_x, train_y, groups, k=5)):\n",
    "    # dev_ind, val_ind는 list 형태로 들어가서 Series형식의 id, Image id를 뽑는다.\n",
    "    dev_y, val_y = train_y[dev_ind], train_y[val_ind] # train index, \n",
    "    dev_groups, val_groups = groups[dev_ind], groups[val_ind] # Image id index,\n",
    "    dev_id, val_id = train_x[dev_ind], train_x[val_ind]\n",
    "\n",
    "    assert len(set(dev_groups) & set(val_groups)) == 0 # 가정 설정문,  동일한게 image file이 있는 지 확인 True이면 그대로 진행 아니라면 Assertion Error 생성\n",
    "    \n",
    "    distrs.append(get_distribution(dev_y))\n",
    "    index.append(f'development set - fold {fold_ind}')\n",
    "    distrs.append(get_distribution(val_y))\n",
    "    index.append(f'validation set - fold {fold_ind}')\n",
    "\n",
    "    # if fold_ind == 0:\n",
    "    #     print(dev_id)\n",
    "    #     dev_id_json = list(set(dev_id))\n",
    "    #     val_id_json = list(set(val_id))\n",
    "    \n",
    "    #     make_json(fold_ind, dev_id_json, val_id_json)\n",
    "    # else:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distribution per class:')\n",
    "pd.DataFrame(distrs, index=index, columns=[f'Label {l}' for l in range(np.max(train_y) + 1)]) # label의 구성 퍼센티지들을 나타낸다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d36e052b391be8c28b05838ade06426769a29575d5fe21a7bc69c7dec0c04c06"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('segmentation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
